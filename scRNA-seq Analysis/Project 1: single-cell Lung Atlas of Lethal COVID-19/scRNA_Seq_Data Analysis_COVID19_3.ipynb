{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data came from https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE171524 --> GSE171524_RAW.tar file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = r\"GSM5226574_C51ctr_raw_counts.csv.gz\"\n",
    "\n",
    "# Read the compressed CSV file\n",
    "df = pd.read_csv(file_path, compression='gzip')\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())  # This will take 55 sec to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scanpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "\n",
    "file_path = r\"GSM5226574_C51ctr_raw_counts.csv.gz\"\n",
    "\n",
    "# Read the compressed CSV file into an AnnData object\n",
    "adata = sc.read_csv(file_path)\n",
    "\n",
    "# Transpose the data to have cells as rows and genes as columns\n",
    "adata = adata.T\n",
    "\n",
    "# Display the contents of the AnnData object\n",
    "print(adata)  # This will take about 60 sec to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell Barcodes\n",
    "adata.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# genes\n",
    "adata.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.X.shape # Number of cells by the number of genes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doublet Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are making single cell libaries, somteimes two or more cells can end up in the same drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scvi-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata # shows the dimmensions of the cells by genes\n",
    "# need to filter down the genes and test if a cell is a doublet or not, so lets narrow the 34546"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.filter_genes(adata, min_cells= 10) # keep genes that are found in at least 10 cells\n",
    "adata # now we are at 19896 genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the top 2000 variable genes\n",
    "sc.pp.highly_variable_genes(adata, n_top_genes=2000, subset=True, flavor='seurat_v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata #observe how there are now only 2000 genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model setup\n",
    "scvi.model.SCVI.setup_anndata(adata)\n",
    "# train vae model\n",
    "vae = scvi.model.SCVI(adata)\n",
    "vae.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the solo model, which predicts doublets\n",
    "solo = scvi.external.SOLO.from_scvi_model(vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solo.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solo.predict()\n",
    "# cell barcode on the left\n",
    "# score for doublet and singlet (higher scores represent prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass soft=False to make a new column that is the predicted label (doublet or singlet)\n",
    "df = solo.predict()\n",
    "df['prediction'] = solo.predict(soft=False)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many doublets and singlets we have\n",
    "df.groupby('prediction').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new column that is the difference in probability of doublet and singlet columns\n",
    "df['dif'] = df.doublet - df.singlet\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df[df.prediction == 'doublet'], x='dif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram shown presents x-axis as the difference in prediction (doublet probability - singlet probability).\n",
    "The y-axis is the count of cell barcodes in that bin (difference).\n",
    "The histogram overall seems to be leveled, however, the distribution begins to drop dramatically than taper off at x = 0.25\n",
    "This suggests that the cell barcodes at x < 0.25 can still likely be singlets, which we do not want to remove.\n",
    "So we will make the cutoff at x = 0.25 keeping 1 < x > 0.25; these are the doublets, with a new dataframe reload the raw cell barcode file and label the matching cell barcodes (from the processed dataframe) as doublets and eliminate them... thus keeping only singlets in the new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doublets = df[(df.prediction == 'doublet') & (df.dif > 0.24)]\n",
    "doublets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the compressed CSV file into an AnnData object\n",
    "adata = sc.read_csv(file_path)\n",
    "\n",
    "# Transpose the data to have cells as rows and genes as columns\n",
    "adata = adata.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs['doublet'] = adata.obs.index.isin(doublets.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~ means you keep the \"False\"\n",
    "adata = adata[~adata.obs.doublet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata # 5424 cell barcodes that are now only singlets\n",
    "# this makes sense as the doublet list was 675 cell barcodes, the raw data was 6099 cell barcodes. (6099-675=5424) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats, the doublets are now removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gene names\n",
    "adata.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mitochondrial genes are annotated as \"MT-\" in the gene names, lets filter that.\n",
    "adata.var['mt'] = adata.var.index.str.startswith('MT-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to now label ribosomal genes\n",
    "# we need to import a list of known ribosomal genes (import from the Broad Institute) can import into pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ribo_url = \"http://software.broadinstitute.org/gsea/msigdb/download_geneset.jsp?geneSetName=KEGG_RIBOSOME&fileType=txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ribo_url = \"http://software.broadinstitute.org/gsea/msigdb/download_geneset.jsp?geneSetName=KEGG_RIBOSOME&fileType=txt\"\n",
    "ribo_genes = pd.read_table(ribo_url, skiprows=2, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ribo_genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ribo_genes[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as we did with mitochondrial genes, we label it to our main dataframe\n",
    "adata.var['ribo'] = adata.var_names.isin(ribo_genes[0].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate qc metrics\n",
    "sc.pp.calculate_qc_metrics(adata, qc_vars=['mt', 'ribo'], percent_top=None, log1p=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pct_dropout_by_counts: percent dropout by counts\n",
    "n_cells_by_counts = gene was found in ___ many cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "total_counts_mt = mitochondrial counts\n",
    "pct_counts_mt = percent of the mitochondrial reads for that given cell\n",
    "pct_counts_ribo = percent counts of the ribosomal reads for that given cell\n",
    "n_genes_by_counts = number of genes positive in that cell\n",
    "total_counts = total number of umis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.var.sort_values('n_cells_by_counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [n_cells_by_counts], you can see some genes like MALAT1 is found in nearly every cell, and other genes like AL445072.1 are found in no cells.\n",
    "We are gonna filter out genes that were not found in at least 3 cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.filter_genes(adata, min_cells=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.var.sort_values('n_cells_by_counts')\n",
    "# now every gene listed is in at least 3 cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets now pre-process adata.obs\n",
    "adata.obs # look at total_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs.sort_values('total_counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lowest total_counts is 401, it seems as if the authers who published these cell barcodes already processed the data to filter out cells that 400 count or fewer. Thus the data is already processed and will not need to filter it ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if the data didnt processed or filter out the cell barcodes we would do the following: sc.pp.filter_cells(adata, min_genes=401)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.violin(adata, ['n_genes_by_counts', 'total_counts', 'pct_counts_mt', 'pct_counts_ribo'],\n",
    "             jitter=0.4, multi_panel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use these QC metrics to eliminate outliers. In 'n_genes_by_counts', if a cell has significantly higher counts of genes than the average, there's a chance that its some artifact, likewise with 'total_counts'. If there is a high mitochondrial percentage, then it could be a sequencing artifact or the cell could have been dying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'pct_counts_mt': there is no percent counts above 10 detected.\n",
    "'pct_counts_ribo': there are most percent counts near 0, some at most percent count is 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use numpy to filter genes by the 98 percentile of 'value'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "upper_lim = np.quantile(adata.obs.n_genes_by_counts.values, 0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_lim # 'value' or the y-axis cutoff for filtering\n",
    "# you can also do \"upper_lim = 3000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = adata[adata.obs.n_genes_by_counts < upper_lim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs # n_genes_by_counts should all be less than 2216.24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = adata[adata.obs.pct_counts_mt < 20] # 20 percent cutoff, will not get rid of any mitochondrial cells but good practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = adata[adata.obs.pct_counts_ribo < 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In single-cell RNA sequencing, there is alot of variation between cells, even between the same cell type due to sequencing biases, etc. Normaliztion will help compare cells and compare genes appropriately without the wide variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.X.sum(axis=1) #total counts of cells, we need to normalize the counts for each cells so that their total counts adds up to the same value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.normalize_total(adata, target_sum=1e4) # normalize every cell to 10,000 UMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.X.sum(axis=1) # each value (cell) got modified based on the starting number of counts for that cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.log1p(adata) #change to log counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.X.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now need to freeze the data as it is now before we begin filtering based on variable genes and regressing and scaling out data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.raw = adata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.highly_variable_genes(adata, n_top_genes=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.highly_variable_genes(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the non-highly variable genes\n",
    "adata = adata[:, adata.var.highly_variable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata #we now have only the top 2000 genes that are highly variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're going to regress out the differences that arise due to the total number of counts mitochondrial counts and the ribosomal counts\n",
    "#  this will filter out some data that are due to processing and just sample quality, sequencing artifact, etc.\n",
    "sc.pp.regress_out(adata, ['total_counts', 'pct_counts_mt', 'pct_counts_ribo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize each gene to the unit variance of that gene\n",
    "sc.pp.scale(adata, max_value=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run component analysis to further reduce the dimensions of the data\n",
    "sc.tl.pca(adata, svd_solver='arpack') # this calculates 50 pcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.pca_variance_ratio(adata, log=True, n_pcs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of how much these pcs contribute. \n",
    "We want to look for where the data points tapers off to where values dont make a y-value difference anymore as you increase pcs value; this is at x = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.neighbors(adata, n_pcs= 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.tl.umap(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(adata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This map are single cells but they haven't been assigned clusters yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leiden algorithm\n",
    "!pip install leidenalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.tl.leiden(adata, resolution = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(adata, color = ['leiden'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now plotted our single cells and labeled them by their clusters using the leiden algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only necessary if you are runing an analysis with multiple samples. The experiment sampled 19 COVID-19 patients and 7 control patients. There are a total of 26 samples in this analysis, thus, integration is necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are essentially gonna now do the same thing as we did for (1) loading the data, (2) doublet removal, (3) preprocessing, and (4) Clustering.\n",
    "However, we will do this for all samples, to make this more efficient, we made this into a function and have the function iterate through each sample file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import scanpy as sc\n",
    "import scvi\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def pp(csv_path):\n",
    "    \n",
    "    # Continue with your analysis using Scanpy\n",
    "    adata = sc.read_csv(csv_path).T\n",
    "    sc.pp.filter_genes(adata, min_cells = 10)\n",
    "    sc.pp.highly_variable_genes(adata, n_top_genes = 2000, subset = True, flavor = 'seurat_v3')\n",
    "    scvi.model.SCVI.setup_anndata(adata)\n",
    "    vae = scvi.model.SCVI(adata)\n",
    "    vae.train()\n",
    "    solo = scvi.external.SOLO.from_scvi_model(vae)\n",
    "    solo.train()\n",
    "    df = solo.predict()\n",
    "    df['prediction'] = solo.predict(soft = False)\n",
    "    df.index = df.index.map(lambda x: x[:-2])\n",
    "    df['dif'] = df.doublet - df.singlet\n",
    "    doublets = df[(df.prediction == 'doublet') & (df.dif > 0.25)]\n",
    "    \n",
    "    adata = sc.read_csv(csv_path).T\n",
    "    adata.obs['Sample'] = csv_path.split('_')[2] #'raw_counts/<sample_identifier>_raw_counts.csv'\n",
    "    \n",
    "    adata.obs['doublet'] = adata.obs.index.isin(doublets.index)\n",
    "    adata = adata[~adata.obs.doublet]\n",
    "    \n",
    "    \n",
    "    sc.pp.filter_cells(adata, min_genes=200) #get rid of cells with fewer than 200 genes\n",
    "    #sc.pp.filter_genes(adata, min_cells=3) #get rid of genes that are found in fewer than 3 cells\n",
    "    adata.var['mt'] = adata.var_names.str.startswith('mt-')  # annotate the group of mitochondrial genes as 'mt'\n",
    "    \n",
    "    ribo_url = \"http://software.broadinstitute.org/gsea/msigdb/download_geneset.jsp?geneSetName=KEGG_RIBOSOME&fileType=txt\"\n",
    "    ribo_genes = pd.read_table(ribo_url, skiprows=2, header=None)\n",
    "    \n",
    "    adata.var['ribo'] = adata.var_names.isin(ribo_genes[0].values)\n",
    "    sc.pp.calculate_qc_metrics(adata, qc_vars=['mt', 'ribo'], percent_top=None, log1p=False, inplace=True)\n",
    "    upper_lim = np.quantile(adata.obs.n_genes_by_counts.values, .98)\n",
    "    adata = adata[adata.obs.n_genes_by_counts < upper_lim]\n",
    "    adata = adata[adata.obs.pct_counts_mt < 20]\n",
    "    adata = adata[adata.obs.pct_counts_ribo < 2]\n",
    "    \n",
    "    return adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "out = []\n",
    "for file in os.listdir('GSE171524_RAW/'):\n",
    "    file_path = os.path.join('GSE171524_RAW', file)  # Construct the full file path\n",
    "    out.append(pp(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import scanpy as sc\n",
    "import scvi\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def pp(csv_path):\n",
    "    adata = sc.read_csv(csv_path).T\n",
    "    sc.pp.filter_genes(adata, min_cells=10)\n",
    "    sc.pp.highly_variable_genes(adata, n_top_genes=2000, subset=True, flavor='seurat_v3')\n",
    "    scvi.model.SCVI.setup_anndata(adata)\n",
    "    vae = scvi.model.SCVI(adata)\n",
    "    vae.train()\n",
    "    solo = scvi.external.SOLO.from_scvi_model(vae)\n",
    "    solo.train()\n",
    "    df = solo.predict()\n",
    "    df['prediction'] = solo.predict(soft=False)\n",
    "    df.index = df.index.map(lambda x: x[:-2])\n",
    "    df['dif'] = df.doublet - df.singlet\n",
    "    doublets = df[(df.prediction == 'doublet') & (df.dif > 0.25)]\n",
    "    \n",
    "    adata = sc.read_csv(csv_path).T\n",
    "    adata.obs['Sample'] = csv_path.split('_')[2]  # Adjust based on your file naming convention\n",
    "    adata.obs['doublet'] = adata.obs.index.isin(doublets.index)\n",
    "    adata = adata[~adata.obs.doublet]\n",
    "    \n",
    "    sc.pp.filter_cells(adata, min_genes=200)\n",
    "    adata.var['mt'] = adata.var_names.str.startswith('mt-')\n",
    "    \n",
    "    ribo_url = \"http://software.broadinstitute.org/gsea/msigdb/download_geneset.jsp?geneSetName=KEGG_RIBOSOME&fileType=txt\"\n",
    "    ribo_genes = pd.read_table(ribo_url, skiprows=2, header=None)\n",
    "    adata.var['ribo'] = adata.var_names.isin(ribo_genes[0].values)\n",
    "    \n",
    "    sc.pp.calculate_qc_metrics(adata, qc_vars=['mt', 'ribo'], percent_top=None, log1p=False, inplace=True)\n",
    "    \n",
    "    upper_lim = np.quantile(adata.obs.n_genes_by_counts.values, .98)\n",
    "    adata = adata[adata.obs.n_genes_by_counts < upper_lim]\n",
    "    adata = adata[adata.obs.pct_counts_mt < 20]\n",
    "    adata = adata[adata.obs.pct_counts_ribo < 2]\n",
    "    \n",
    "    return adata\n",
    "\n",
    "def batch_process(input_dir, output_dir, batch_size=6):\n",
    "    files = os.listdir(input_dir)\n",
    "    batches = [files[i:i + batch_size] for i in range(0, len(files), batch_size)]\n",
    "    \n",
    "    for i, batch in enumerate(batches):\n",
    "        out = []\n",
    "        for file in batch:\n",
    "            file_path = os.path.join(input_dir, file)\n",
    "            out.append(pp(file_path))\n",
    "        \n",
    "        batch_adata = sc.concat(out)\n",
    "        batch_adata.write(os.path.join(output_dir, f'batch_{i}.h5ad'))\n",
    "        print(f'Processed batch {i} and saved to disk.')\n",
    "\n",
    "# Set your directories and batch size\n",
    "input_dir = 'GSE171524_RAW/'\n",
    "output_dir = 'processed_batches/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Run batch processing\n",
    "batch_process(input_dir, output_dir, batch_size=6)\n",
    "\n",
    "# Later, you can concatenate the batches\n",
    "batches = [sc.read(os.path.join(output_dir, f)) for f in os.listdir(output_dir) if f.endswith('.h5ad')]\n",
    "adata = sc.concat(batches)\n",
    "sc.pp.filter_genes(adata, min_cells=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import scanpy as sc\n",
    "import scvi\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def pp(csv_path):\n",
    "    adata = sc.read_csv(csv_path).T\n",
    "    sc.pp.filter_genes(adata, min_cells=10)\n",
    "    sc.pp.highly_variable_genes(adata, n_top_genes=2000, subset=True, flavor='seurat_v3')\n",
    "    scvi.model.SCVI.setup_anndata(adata)\n",
    "    vae = scvi.model.SCVI(adata)\n",
    "    vae.train()\n",
    "    solo = scvi.external.SOLO.from_scvi_model(vae)\n",
    "    solo.train()\n",
    "    df = solo.predict()\n",
    "    df['prediction'] = solo.predict(soft=False)\n",
    "    df.index = df.index.map(lambda x: x[:-2])\n",
    "    df['dif'] = df.doublet - df.singlet\n",
    "    doublets = df[(df.prediction == 'doublet') & (df.dif > 0.25)]\n",
    "    \n",
    "    adata = sc.read_csv(csv_path).T\n",
    "    adata.obs['Sample'] = csv_path.split('_')[2]  # Adjust based on your file naming convention\n",
    "    adata.obs['doublet'] = adata.obs.index.isin(doublets.index)\n",
    "    adata = adata[~adata.obs.doublet]\n",
    "    \n",
    "    sc.pp.filter_cells(adata, min_genes=200)\n",
    "    adata.var['mt'] = adata.var_names.str.startswith('mt-')\n",
    "    \n",
    "    ribo_url = \"http://software.broadinstitute.org/gsea/msigdb/download_geneset.jsp?geneSetName=KEGG_RIBOSOME&fileType=txt\"\n",
    "    ribo_genes = pd.read_table(ribo_url, skiprows=2, header=None)\n",
    "    adata.var['ribo'] = adata.var_names.isin(ribo_genes[0].values)\n",
    "    \n",
    "    sc.pp.calculate_qc_metrics(adata, qc_vars=['mt', 'ribo'], percent_top=None, log1p=False, inplace=True)\n",
    "    \n",
    "    upper_lim = np.quantile(adata.obs.n_genes_by_counts.values, .98)\n",
    "    adata = adata[adata.obs.n_genes_by_counts < upper_lim]\n",
    "    adata = adata[adata.obs.pct_counts_mt < 20]\n",
    "    adata = adata[adata.obs.pct_counts_ribo < 2]\n",
    "    \n",
    "    return adata\n",
    "\n",
    "def batch_process(input_dir, output_dir, batch_size=4):\n",
    "    files = os.listdir(input_dir)\n",
    "    batches = [files[i:i + batch_size] for i in range(0, len(files), batch_size)]\n",
    "    \n",
    "    for i, batch in enumerate(batches):\n",
    "        out = []\n",
    "        for file in batch:\n",
    "            file_path = os.path.join(input_dir, file)\n",
    "            out.append(pp(file_path))\n",
    "        \n",
    "        batch_adata = sc.concat(out)\n",
    "        batch_adata.write(os.path.join(output_dir, f'batch_{i}.h5ad'))\n",
    "        print(f'Processed batch {i} and saved to disk.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your directories and batch size\n",
    "input_dir = 'GSE171524_RAW/'\n",
    "output_dir = 'D:/processed_batches/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Run batch processing\n",
    "batch_process(input_dir, output_dir, batch_size=4)\n",
    "\n",
    "# Later, you can concatenate the batches\n",
    "batches = [sc.read(os.path.join(output_dir, f)) for f in os.listdir(output_dir) if f.endswith('.h5ad')]\n",
    "adata = sc.concat(batches)\n",
    "sc.pp.filter_genes(adata, min_cells=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gc  # Import garbage collector\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import scanpy as sc\n",
    "import scvi\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def pp(csv_path):\n",
    "    adata = sc.read_csv(csv_path).T\n",
    "    sc.pp.filter_genes(adata, min_cells=10)\n",
    "    sc.pp.highly_variable_genes(adata, n_top_genes=2000, subset=True, flavor='seurat_v3')\n",
    "    scvi.model.SCVI.setup_anndata(adata)\n",
    "    vae = scvi.model.SCVI(adata)\n",
    "    vae.train()\n",
    "    solo = scvi.external.SOLO.from_scvi_model(vae)\n",
    "    solo.train()\n",
    "    df = solo.predict()\n",
    "    df['prediction'] = solo.predict(soft=False)\n",
    "    df.index = df.index.map(lambda x: x[:-2])\n",
    "    df['dif'] = df.doublet - df.singlet\n",
    "    doublets = df[(df.prediction == 'doublet') & (df.dif > 0.25)]\n",
    "    \n",
    "    adata = sc.read_csv(csv_path).T\n",
    "    adata.obs['Sample'] = csv_path.split('_')[2]\n",
    "    adata.obs['doublet'] = adata.obs.index.isin(doublets.index)\n",
    "    adata = adata[~adata.obs.doublet]\n",
    "    \n",
    "    sc.pp.filter_cells(adata, min_genes=200)\n",
    "    adata.var['mt'] = adata.var_names.str.startswith('mt-')\n",
    "    \n",
    "    ribo_url = \"http://software.broadinstitute.org/gsea/msigdb/download_geneset.jsp?geneSetName=KEGG_RIBOSOME&fileType=txt\"\n",
    "    ribo_genes = pd.read_table(ribo_url, skiprows=2, header=None)\n",
    "    adata.var['ribo'] = adata.var_names.isin(ribo_genes[0].values)\n",
    "    \n",
    "    sc.pp.calculate_qc_metrics(adata, qc_vars=['mt', 'ribo'], percent_top=None, log1p=False, inplace=True)\n",
    "    \n",
    "    upper_lim = np.quantile(adata.obs.n_genes_by_counts.values, .98)\n",
    "    adata = adata[adata.obs.n_genes_by_counts < upper_lim]\n",
    "    adata = adata[adata.obs.pct_counts_mt < 20]\n",
    "    adata = adata[adata.obs.pct_counts_ribo < 2]\n",
    "    \n",
    "    return adata\n",
    "\n",
    "def batch_process(input_dir, output_dir, batch_size=2):  # Reduce batch size to 2\n",
    "    files = os.listdir(input_dir)\n",
    "    batches = [files[i:i + batch_size] for i in range(0, len(files), batch_size)]\n",
    "    \n",
    "    for i, batch in enumerate(batches):\n",
    "        out = []\n",
    "        for file in batch:\n",
    "            file_path = os.path.join(input_dir, file)\n",
    "            out.append(pp(file_path))\n",
    "        \n",
    "        batch_adata = sc.concat(out)\n",
    "        batch_adata.write(os.path.join(output_dir, f'batch_{i}.h5ad'))\n",
    "        print(f'Processed batch {i} and saved to disk.')\n",
    "        \n",
    "        del out, batch_adata  # Clear variables to free memory\n",
    "        gc.collect()  # Force garbage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [15:05<00:00,  2.23s/it, v_num=1, train_loss_step=331, train_loss_epoch=323]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [15:06<00:00,  2.27s/it, v_num=1, train_loss_step=331, train_loss_epoch=323]\n",
      "\u001b[34mINFO    \u001b[0m Creating doublets, preparing SOLO model.                                                                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167/400:  42%|████▏     | 167/400 [01:02<01:26,  2.68it/s, v_num=1, train_loss_step=0.316, train_loss_epoch=0.3]  \n",
      "Monitored metric validation_loss did not improve in the last 30 records. Best score: 0.285. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\scanpy\\preprocessing\\_simple.py:166: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
      "  adata.obs[\"n_genes\"] = number\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [11:13<00:00,  1.71s/it, v_num=1, train_loss_step=405, train_loss_epoch=396]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [11:13<00:00,  1.68s/it, v_num=1, train_loss_step=405, train_loss_epoch=396]\n",
      "\u001b[34mINFO    \u001b[0m Creating doublets, preparing SOLO model.                                                                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143/400:  36%|███▌      | 143/400 [00:40<01:13,  3.51it/s, v_num=1, train_loss_step=0.357, train_loss_epoch=0.306] \n",
      "Monitored metric validation_loss did not improve in the last 30 records. Best score: 0.281. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\scanpy\\preprocessing\\_simple.py:166: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
      "  adata.obs[\"n_genes\"] = number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 0 and saved to disk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [17:35<00:00,  2.61s/it, v_num=1, train_loss_step=389, train_loss_epoch=331]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [17:35<00:00,  2.64s/it, v_num=1, train_loss_step=389, train_loss_epoch=331]\n",
      "\u001b[34mINFO    \u001b[0m Creating doublets, preparing SOLO model.                                                                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 303/400:  76%|███████▌  | 303/400 [02:08<00:41,  2.36it/s, v_num=1, train_loss_step=0.562, train_loss_epoch=0.31] \n",
      "Monitored metric validation_loss did not improve in the last 30 records. Best score: 0.292. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\scanpy\\preprocessing\\_simple.py:166: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
      "  adata.obs[\"n_genes\"] = number\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [10:50<00:00,  1.61s/it, v_num=1, train_loss_step=295, train_loss_epoch=308]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [10:50<00:00,  1.63s/it, v_num=1, train_loss_step=295, train_loss_epoch=308]\n",
      "\u001b[34mINFO    \u001b[0m Creating doublets, preparing SOLO model.                                                                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163/400:  41%|████      | 163/400 [00:43<01:03,  3.73it/s, v_num=1, train_loss_step=0.195, train_loss_epoch=0.247]\n",
      "Monitored metric validation_loss did not improve in the last 30 records. Best score: 0.237. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\scanpy\\preprocessing\\_simple.py:166: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
      "  adata.obs[\"n_genes\"] = number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 1 and saved to disk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [14:06<00:00,  2.12s/it, v_num=1, train_loss_step=311, train_loss_epoch=305]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [14:06<00:00,  2.12s/it, v_num=1, train_loss_step=311, train_loss_epoch=305]\n",
      "\u001b[34mINFO    \u001b[0m Creating doublets, preparing SOLO model.                                                                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/400:  39%|███▉      | 155/400 [00:53<01:24,  2.89it/s, v_num=1, train_loss_step=0.224, train_loss_epoch=0.228]\n",
      "Monitored metric validation_loss did not improve in the last 30 records. Best score: 0.247. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\scanpy\\preprocessing\\_simple.py:166: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
      "  adata.obs[\"n_genes\"] = number\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [10:08<00:00,  1.52s/it, v_num=1, train_loss_step=330, train_loss_epoch=326]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [10:08<00:00,  1.52s/it, v_num=1, train_loss_step=330, train_loss_epoch=326]\n",
      "\u001b[34mINFO    \u001b[0m Creating doublets, preparing SOLO model.                                                                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 231/400:  58%|█████▊    | 231/400 [00:58<00:42,  3.98it/s, v_num=1, train_loss_step=0.219, train_loss_epoch=0.255] \n",
      "Monitored metric validation_loss did not improve in the last 30 records. Best score: 0.247. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\scanpy\\preprocessing\\_simple.py:166: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
      "  adata.obs[\"n_genes\"] = number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 2 and saved to disk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [11:45<00:00,  1.76s/it, v_num=1, train_loss_step=291, train_loss_epoch=288]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [11:45<00:00,  1.76s/it, v_num=1, train_loss_step=291, train_loss_epoch=288]\n",
      "\u001b[34mINFO    \u001b[0m Creating doublets, preparing SOLO model.                                                                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 243/400:  61%|██████    | 243/400 [01:10<00:45,  3.43it/s, v_num=1, train_loss_step=0.0697, train_loss_epoch=0.251]\n",
      "Monitored metric validation_loss did not improve in the last 30 records. Best score: 0.250. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\scanpy\\preprocessing\\_simple.py:166: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
      "  adata.obs[\"n_genes\"] = number\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [07:34<00:00,  1.15s/it, v_num=1, train_loss_step=397, train_loss_epoch=332]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [07:34<00:00,  1.14s/it, v_num=1, train_loss_step=397, train_loss_epoch=332]\n",
      "\u001b[34mINFO    \u001b[0m Creating doublets, preparing SOLO model.                                                                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 208/400:  52%|█████▏    | 208/400 [00:39<00:36,  5.24it/s, v_num=1, train_loss_step=0.204, train_loss_epoch=0.272]\n",
      "Monitored metric validation_loss did not improve in the last 30 records. Best score: 0.297. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\scanpy\\preprocessing\\_simple.py:166: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
      "  adata.obs[\"n_genes\"] = number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 3 and saved to disk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [12:28<00:00,  1.88s/it, v_num=1, train_loss_step=537, train_loss_epoch=473]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [12:28<00:00,  1.87s/it, v_num=1, train_loss_step=537, train_loss_epoch=473]\n",
      "\u001b[34mINFO    \u001b[0m Creating doublets, preparing SOLO model.                                                                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 215/400:  54%|█████▍    | 215/400 [01:05<00:56,  3.28it/s, v_num=1, train_loss_step=0.302, train_loss_epoch=0.352]\n",
      "Monitored metric validation_loss did not improve in the last 30 records. Best score: 0.358. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\scanpy\\preprocessing\\_simple.py:166: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
      "  adata.obs[\"n_genes\"] = number\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [09:01<00:00,  1.34s/it, v_num=1, train_loss_step=260, train_loss_epoch=259]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [09:01<00:00,  1.35s/it, v_num=1, train_loss_step=260, train_loss_epoch=259]\n",
      "\u001b[34mINFO    \u001b[0m Creating doublets, preparing SOLO model.                                                                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 202/400:  50%|█████     | 202/400 [00:44<00:43,  4.51it/s, v_num=1, train_loss_step=0.315, train_loss_epoch=0.275]\n",
      "Monitored metric validation_loss did not improve in the last 30 records. Best score: 0.292. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\scanpy\\preprocessing\\_simple.py:166: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
      "  adata.obs[\"n_genes\"] = number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 4 and saved to disk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [07:30<00:00,  1.13s/it, v_num=1, train_loss_step=341, train_loss_epoch=344]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [07:30<00:00,  1.13s/it, v_num=1, train_loss_step=341, train_loss_epoch=344]\n",
      "\u001b[34mINFO    \u001b[0m Creating doublets, preparing SOLO model.                                                                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 273/400:  68%|██████▊   | 273/400 [00:53<00:24,  5.14it/s, v_num=1, train_loss_step=0.467, train_loss_epoch=0.319]\n",
      "Monitored metric validation_loss did not improve in the last 30 records. Best score: 0.325. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\scanpy\\preprocessing\\_simple.py:166: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
      "  adata.obs[\"n_genes\"] = number\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [18:53<00:00,  2.79s/it, v_num=1, train_loss_step=350, train_loss_epoch=361]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [18:53<00:00,  2.83s/it, v_num=1, train_loss_step=350, train_loss_epoch=361]\n",
      "\u001b[34mINFO    \u001b[0m Creating doublets, preparing SOLO model.                                                                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180/400:  45%|████▌     | 180/400 [01:21<01:39,  2.22it/s, v_num=1, train_loss_step=0.376, train_loss_epoch=0.299]\n",
      "Monitored metric validation_loss did not improve in the last 30 records. Best score: 0.284. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\scanpy\\preprocessing\\_simple.py:166: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
      "  adata.obs[\"n_genes\"] = number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 5 and saved to disk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [12:35<00:00,  1.87s/it, v_num=1, train_loss_step=391, train_loss_epoch=339]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [12:35<00:00,  1.89s/it, v_num=1, train_loss_step=391, train_loss_epoch=339]\n",
      "\u001b[34mINFO    \u001b[0m Creating doublets, preparing SOLO model.                                                                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299/400:  75%|███████▍  | 299/400 [01:32<00:31,  3.25it/s, v_num=1, train_loss_step=0.474, train_loss_epoch=0.355] \n",
      "Monitored metric validation_loss did not improve in the last 30 records. Best score: 0.318. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\scanpy\\preprocessing\\_simple.py:166: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
      "  adata.obs[\"n_genes\"] = number\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [10:18<00:00,  1.54s/it, v_num=1, train_loss_step=371, train_loss_epoch=341]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [10:18<00:00,  1.55s/it, v_num=1, train_loss_step=371, train_loss_epoch=341]\n",
      "\u001b[34mINFO    \u001b[0m Creating doublets, preparing SOLO model.                                                                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 230/400:  57%|█████▊    | 230/400 [00:58<00:42,  3.96it/s, v_num=1, train_loss_step=0.293, train_loss_epoch=0.285]\n",
      "Monitored metric validation_loss did not improve in the last 30 records. Best score: 0.281. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\scanpy\\preprocessing\\_simple.py:166: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
      "  adata.obs[\"n_genes\"] = number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 6 and saved to disk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [08:52<00:00,  1.34s/it, v_num=1, train_loss_step=342, train_loss_epoch=339]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [08:52<00:00,  1.33s/it, v_num=1, train_loss_step=342, train_loss_epoch=339]\n",
      "\u001b[34mINFO    \u001b[0m Creating doublets, preparing SOLO model.                                                                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180/400:  45%|████▌     | 180/400 [00:40<00:49,  4.41it/s, v_num=1, train_loss_step=0.391, train_loss_epoch=0.253] \n",
      "Monitored metric validation_loss did not improve in the last 30 records. Best score: 0.247. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\scanpy\\preprocessing\\_simple.py:166: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
      "  adata.obs[\"n_genes\"] = number\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [03:51<00:00,  1.76it/s, v_num=1, train_loss_step=316, train_loss_epoch=310]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [03:51<00:00,  1.73it/s, v_num=1, train_loss_step=316, train_loss_epoch=310]\n",
      "\u001b[34mINFO    \u001b[0m Creating doublets, preparing SOLO model.                                                                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 217/400:  54%|█████▍    | 217/400 [00:21<00:18,  9.87it/s, v_num=1, train_loss_step=0.286, train_loss_epoch=0.286]\n",
      "Monitored metric validation_loss did not improve in the last 30 records. Best score: 0.296. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\scanpy\\preprocessing\\_simple.py:166: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
      "  adata.obs[\"n_genes\"] = number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 7 and saved to disk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [08:06<00:00,  1.24s/it, v_num=1, train_loss_step=266, train_loss_epoch=356]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [08:06<00:00,  1.22s/it, v_num=1, train_loss_step=266, train_loss_epoch=356]\n",
      "\u001b[34mINFO    \u001b[0m Creating doublets, preparing SOLO model.                                                                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 256/400:  64%|██████▍   | 256/400 [00:52<00:29,  4.92it/s, v_num=1, train_loss_step=0.327, train_loss_epoch=0.343]\n",
      "Monitored metric validation_loss did not improve in the last 30 records. Best score: 0.336. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\scanpy\\preprocessing\\_simple.py:166: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
      "  adata.obs[\"n_genes\"] = number\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [09:32<00:00,  1.53s/it, v_num=1, train_loss_step=320, train_loss_epoch=318]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [09:32<00:00,  1.43s/it, v_num=1, train_loss_step=320, train_loss_epoch=318]\n",
      "\u001b[34mINFO    \u001b[0m Creating doublets, preparing SOLO model.                                                                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 226/400:  56%|█████▋    | 226/400 [00:53<00:41,  4.23it/s, v_num=1, train_loss_step=0.4, train_loss_epoch=0.317]  \n",
      "Monitored metric validation_loss did not improve in the last 30 records. Best score: 0.327. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\scanpy\\preprocessing\\_simple.py:166: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
      "  adata.obs[\"n_genes\"] = number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 8 and saved to disk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [11:56<00:00,  1.80s/it, v_num=1, train_loss_step=437, train_loss_epoch=370]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [11:56<00:00,  1.79s/it, v_num=1, train_loss_step=437, train_loss_epoch=370]\n",
      "\u001b[34mINFO    \u001b[0m Creating doublets, preparing SOLO model.                                                                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 295/400:  74%|███████▍  | 295/400 [01:31<00:32,  3.23it/s, v_num=1, train_loss_step=0.299, train_loss_epoch=0.298]\n",
      "Monitored metric validation_loss did not improve in the last 30 records. Best score: 0.290. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\scanpy\\preprocessing\\_simple.py:166: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
      "  adata.obs[\"n_genes\"] = number\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [10:02<00:00,  1.49s/it, v_num=1, train_loss_step=308, train_loss_epoch=353]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [10:02<00:00,  1.51s/it, v_num=1, train_loss_step=308, train_loss_epoch=353]\n",
      "\u001b[34mINFO    \u001b[0m Creating doublets, preparing SOLO model.                                                                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122/400:  30%|███       | 122/400 [00:30<01:08,  4.04it/s, v_num=1, train_loss_step=0.284, train_loss_epoch=0.295]\n",
      "Monitored metric validation_loss did not improve in the last 30 records. Best score: 0.300. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\scanpy\\preprocessing\\_simple.py:166: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
      "  adata.obs[\"n_genes\"] = number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 9 and saved to disk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [04:28<00:00,  1.50it/s, v_num=1, train_loss_step=402, train_loss_epoch=381]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [04:28<00:00,  1.49it/s, v_num=1, train_loss_step=402, train_loss_epoch=381]\n",
      "\u001b[34mINFO    \u001b[0m Creating doublets, preparing SOLO model.                                                                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 313/400:  78%|███████▊  | 313/400 [00:36<00:10,  8.51it/s, v_num=1, train_loss_step=0.36, train_loss_epoch=0.29]  \n",
      "Monitored metric validation_loss did not improve in the last 30 records. Best score: 0.302. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\scanpy\\preprocessing\\_simple.py:166: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
      "  adata.obs[\"n_genes\"] = number\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [11:24<00:00,  1.71s/it, v_num=1, train_loss_step=321, train_loss_epoch=339]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [11:24<00:00,  1.71s/it, v_num=1, train_loss_step=321, train_loss_epoch=339]\n",
      "\u001b[34mINFO    \u001b[0m Creating doublets, preparing SOLO model.                                                                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 219/400:  55%|█████▍    | 219/400 [01:02<00:51,  3.52it/s, v_num=1, train_loss_step=0.235, train_loss_epoch=0.266] \n",
      "Monitored metric validation_loss did not improve in the last 30 records. Best score: 0.245. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\scanpy\\preprocessing\\_simple.py:166: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
      "  adata.obs[\"n_genes\"] = number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 10 and saved to disk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [06:55<00:00,  1.10s/it, v_num=1, train_loss_step=303, train_loss_epoch=304]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [06:55<00:00,  1.04s/it, v_num=1, train_loss_step=303, train_loss_epoch=304]\n",
      "\u001b[34mINFO    \u001b[0m Creating doublets, preparing SOLO model.                                                                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 330/400:  82%|████████▎ | 330/400 [00:58<00:12,  5.60it/s, v_num=1, train_loss_step=0.162, train_loss_epoch=0.186] \n",
      "Monitored metric validation_loss did not improve in the last 30 records. Best score: 0.212. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\scanpy\\preprocessing\\_simple.py:166: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
      "  adata.obs[\"n_genes\"] = number\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [06:10<00:00,  1.09it/s, v_num=1, train_loss_step=383, train_loss_epoch=417]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [06:10<00:00,  1.08it/s, v_num=1, train_loss_step=383, train_loss_epoch=417]\n",
      "\u001b[34mINFO    \u001b[0m Creating doublets, preparing SOLO model.                                                                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 213/400:  53%|█████▎    | 213/400 [00:33<00:29,  6.39it/s, v_num=1, train_loss_step=0.356, train_loss_epoch=0.351]\n",
      "Monitored metric validation_loss did not improve in the last 30 records. Best score: 0.345. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\scanpy\\preprocessing\\_simple.py:166: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
      "  adata.obs[\"n_genes\"] = number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 11 and saved to disk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [08:28<00:00,  1.27s/it, v_num=1, train_loss_step=319, train_loss_epoch=322]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [08:28<00:00,  1.27s/it, v_num=1, train_loss_step=319, train_loss_epoch=322]\n",
      "\u001b[34mINFO    \u001b[0m Creating doublets, preparing SOLO model.                                                                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227/400:  57%|█████▋    | 227/400 [00:47<00:36,  4.73it/s, v_num=1, train_loss_step=0.439, train_loss_epoch=0.292] \n",
      "Monitored metric validation_loss did not improve in the last 30 records. Best score: 0.283. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\scanpy\\preprocessing\\_simple.py:166: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
      "  adata.obs[\"n_genes\"] = number\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [17:18<00:00,  2.60s/it, v_num=1, train_loss_step=319, train_loss_epoch=341]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [17:18<00:00,  2.60s/it, v_num=1, train_loss_step=319, train_loss_epoch=341]\n",
      "\u001b[34mINFO    \u001b[0m Creating doublets, preparing SOLO model.                                                                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/400:  43%|████▎     | 172/400 [01:11<01:34,  2.41it/s, v_num=1, train_loss_step=0.223, train_loss_epoch=0.314]\n",
      "Monitored metric validation_loss did not improve in the last 30 records. Best score: 0.293. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: UserWarning: Prior to scvi-tools 1.1.3, `SOLO.predict` with `soft=True` (the default option) returned logits instead of probabilities. This behavior has since been corrected to return probabiltiies. The previous behavior can be replicated by passing in `return_logits=True`.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\scanpy\\preprocessing\\_simple.py:166: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
      "  adata.obs[\"n_genes\"] = number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 12 and saved to disk.\n"
     ]
    }
   ],
   "source": [
    "# Set your directories and batch size\n",
    "input_dir = 'GSE171524_RAW/'\n",
    "output_dir = 'D:/processed_batches/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Run batch processing with reduced batch size\n",
    "batch_process(input_dir, output_dir, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.24 GiB for an array with shape (9647, 34546) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Later, you can concatenate the batches\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m batches \u001b[38;5;241m=\u001b[39m [\u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(output_dir) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.h5ad\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[0;32m      3\u001b[0m adata \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mconcat(batches)\n\u001b[0;32m      4\u001b[0m sc\u001b[38;5;241m.\u001b[39mpp\u001b[38;5;241m.\u001b[39mfilter_genes(adata, min_cells\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\legacy_api_wrap\\__init__.py:80\u001b[0m, in \u001b[0;36mlegacy_api.<locals>.wrapper.<locals>.fn_compatible\u001b[1;34m(*args_all, **kw)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn_compatible\u001b[39m(\u001b[38;5;241m*\u001b[39margs_all: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m R:\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args_all) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m n_positional:\n\u001b[1;32m---> 80\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     args_pos: P\u001b[38;5;241m.\u001b[39margs\n\u001b[0;32m     83\u001b[0m     args_pos, args_rest \u001b[38;5;241m=\u001b[39m args_all[:n_positional], args_all[n_positional:]\n",
      "File \u001b[1;32mc:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\scanpy\\readwrite.py:129\u001b[0m, in \u001b[0;36mread\u001b[1;34m(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m filename \u001b[38;5;241m=\u001b[39m Path(filename)  \u001b[38;5;66;03m# allow passing strings\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_valid_filename(filename):\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbacked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbacked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43msheet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msheet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelimiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfirst_column_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_column_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbackup_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackup_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_compression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# generate filename and read to dict\u001b[39;00m\n\u001b[0;32m    142\u001b[0m filekey \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(filename)\n",
      "File \u001b[1;32mc:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\scanpy\\readwrite.py:764\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh5ad\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[0;32m    763\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sheet \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 764\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mread_h5ad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbacked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbacked\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    766\u001b[0m         logg\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreading sheet \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msheet\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\anndata\\_io\\h5ad.py:261\u001b[0m, in \u001b[0;36mread_h5ad\u001b[1;34m(filename, backed, as_sparse, as_sparse_fmt, chunk_size)\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m read_dataframe(elem)\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(elem)\n\u001b[1;32m--> 261\u001b[0m adata \u001b[38;5;241m=\u001b[39m \u001b[43mread_dispatched\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# Backwards compat (should figure out which version)\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw.X\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m f:\n",
      "File \u001b[1;32mc:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\anndata\\experimental\\_dispatch_io.py:48\u001b[0m, in \u001b[0;36mread_dispatched\u001b[1;34m(elem, callback)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01manndata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_io\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _REGISTRY, Reader\n\u001b[0;32m     46\u001b[0m reader \u001b[38;5;241m=\u001b[39m Reader(_REGISTRY, callback\u001b[38;5;241m=\u001b[39mcallback)\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_elem\u001b[49m\u001b[43m(\u001b[49m\u001b[43melem\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\anndata\\_io\\utils.py:207\u001b[0m, in \u001b[0;36mreport_read_key_on_error.<locals>.func_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo element found in args.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    209\u001b[0m     path, key \u001b[38;5;241m=\u001b[39m _get_display_path(store)\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\anndata\\_io\\specs\\registry.py:256\u001b[0m, in \u001b[0;36mReader.read_elem\u001b[1;34m(self, elem, modifiers)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m read_func(elem)\n\u001b[1;32m--> 256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mread_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miospec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miospec\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\anndata\\_io\\h5ad.py:245\u001b[0m, in \u001b[0;36mread_h5ad.<locals>.callback\u001b[1;34m(func, elem_name, elem, iospec)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcallback\u001b[39m(func, elem_name: \u001b[38;5;28mstr\u001b[39m, elem, iospec):\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m iospec\u001b[38;5;241m.\u001b[39mencoding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manndata\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m elem_name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m AnnData(\n\u001b[0;32m    242\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\n\u001b[0;32m    243\u001b[0m                 \u001b[38;5;66;03m# This is covering up backwards compat in the anndata initializer\u001b[39;00m\n\u001b[0;32m    244\u001b[0m                 \u001b[38;5;66;03m# In most cases we should be able to call `func(elen[k])` instead\u001b[39;00m\n\u001b[1;32m--> 245\u001b[0m                 k: \u001b[43mread_dispatched\u001b[49m\u001b[43m(\u001b[49m\u001b[43melem\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m elem\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[0;32m    247\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m k\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    248\u001b[0m             }\n\u001b[0;32m    249\u001b[0m         )\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m elem_name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/raw.\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    251\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\anndata\\experimental\\_dispatch_io.py:48\u001b[0m, in \u001b[0;36mread_dispatched\u001b[1;34m(elem, callback)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01manndata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_io\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _REGISTRY, Reader\n\u001b[0;32m     46\u001b[0m reader \u001b[38;5;241m=\u001b[39m Reader(_REGISTRY, callback\u001b[38;5;241m=\u001b[39mcallback)\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_elem\u001b[49m\u001b[43m(\u001b[49m\u001b[43melem\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\anndata\\_io\\utils.py:207\u001b[0m, in \u001b[0;36mreport_read_key_on_error.<locals>.func_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo element found in args.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    209\u001b[0m     path, key \u001b[38;5;241m=\u001b[39m _get_display_path(store)\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\anndata\\_io\\specs\\registry.py:256\u001b[0m, in \u001b[0;36mReader.read_elem\u001b[1;34m(self, elem, modifiers)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m read_func(elem)\n\u001b[1;32m--> 256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mread_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miospec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miospec\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\anndata\\_io\\h5ad.py:259\u001b[0m, in \u001b[0;36mread_h5ad.<locals>.callback\u001b[1;34m(func, elem_name, elem, iospec)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m elem_name \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/obs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/var\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;66;03m# Backwards compat\u001b[39;00m\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m read_dataframe(elem)\n\u001b[1;32m--> 259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43melem\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\anndata\\_io\\specs\\methods.py:378\u001b[0m, in \u001b[0;36mread_array\u001b[1;34m(elem, _reader)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;129m@_REGISTRY\u001b[39m\u001b[38;5;241m.\u001b[39mregister_read(H5Array, IOSpec(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    375\u001b[0m \u001b[38;5;129m@_REGISTRY\u001b[39m\u001b[38;5;241m.\u001b[39mregister_read(ZarrArray, IOSpec(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    376\u001b[0m \u001b[38;5;129m@_REGISTRY\u001b[39m\u001b[38;5;241m.\u001b[39mregister_read(ZarrArray, IOSpec(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring-array\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_array\u001b[39m(elem, _reader):\n\u001b[1;32m--> 378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43melem\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mh5py\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\h5py\\_hl\\dataset.py:758\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, args, new_dtype)\u001b[0m\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fast_read_ok \u001b[38;5;129;01mand\u001b[39;00m (new_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    757\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 758\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fast_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    759\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    760\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall back to Python read pathway below\u001b[39;00m\n",
      "File \u001b[1;32mh5py\\_selector.pyx:368\u001b[0m, in \u001b[0;36mh5py._selector.Reader.read\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\_selector.pyx:342\u001b[0m, in \u001b[0;36mh5py._selector.Reader.make_array\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.24 GiB for an array with shape (9647, 34546) and data type float32",
      "\u001b[0mError raised while reading key 'X' of <class 'h5py._hl.dataset.Dataset'> from /"
     ]
    }
   ],
   "source": [
    "# Later, you can concatenate the batches\n",
    "batches = [sc.read(os.path.join(output_dir, f)) for f in os.listdir(output_dir) if f.endswith('.h5ad')]\n",
    "adata = sc.concat(batches)\n",
    "sc.pp.filter_genes(adata, min_cells=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sequential Processing with Incremental Merging: Instead of loading all .h5ad files at once, process them one by one or in smaller groups, then combine the results incrementally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch_0.h5ad...\n",
      "Processing batch_1.h5ad...\n",
      "Processing batch_2.h5ad...\n",
      "Processing batch_3.h5ad...\n",
      "Processing batch_4.h5ad...\n",
      "Processing batch_5.h5ad...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 7.08 GiB for an array with shape (55051, 34546) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m     merged_adata \u001b[38;5;241m=\u001b[39m adata\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 12\u001b[0m     merged_adata \u001b[38;5;241m=\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmerged_adata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madata\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m adata\n\u001b[0;32m     15\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[1;32mc:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\anndata\\_core\\merge.py:1298\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise)\u001b[0m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;66;03m# Annotation for other axis\u001b[39;00m\n\u001b[0;32m   1294\u001b[0m alt_annot \u001b[38;5;241m=\u001b[39m merge_dataframes(\n\u001b[0;32m   1295\u001b[0m     [\u001b[38;5;28mgetattr\u001b[39m(a, alt_dim) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m adatas], alt_indices, merge\n\u001b[0;32m   1296\u001b[0m )\n\u001b[1;32m-> 1298\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mconcat_Xs\u001b[49m\u001b[43m(\u001b[49m\u001b[43madatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreindexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m join \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1301\u001b[0m     concat_aligned_mapping \u001b[38;5;241m=\u001b[39m inner_concat_aligned_mapping\n",
      "File \u001b[1;32mc:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\anndata\\_core\\merge.py:1048\u001b[0m, in \u001b[0;36mconcat_Xs\u001b[1;34m(adatas, reindexers, axis, fill_value)\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m   1043\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome (but not all) of the AnnData\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms to be concatenated had no .X value. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1044\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConcatenation is currently only implemented for cases where all or none of\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1045\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m the AnnData\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms have .X assigned.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1046\u001b[0m     )\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1048\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcat_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreindexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\agarc\\anaconda3\\envs\\bioenv\\Lib\\site-packages\\anndata\\_core\\merge.py:826\u001b[0m, in \u001b[0;36mconcat_arrays\u001b[1;34m(arrays, reindexers, axis, index, fill_value)\u001b[0m\n\u001b[0;32m    818\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sparse_stack(\n\u001b[0;32m    819\u001b[0m         [\n\u001b[0;32m    820\u001b[0m             f(as_sparse(a), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m axis, fill_value\u001b[38;5;241m=\u001b[39mfill_value)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    823\u001b[0m         \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    824\u001b[0m     )\n\u001b[0;32m    825\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 826\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m    828\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    829\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreindexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 7.08 GiB for an array with shape (55051, 34546) and data type float32"
     ]
    }
   ],
   "source": [
    "output_dir = 'D:/processed_batches/'\n",
    "merged_adata = None\n",
    "\n",
    "for file in os.listdir(output_dir):\n",
    "    if file.endswith('.h5ad'):\n",
    "        print(f\"Processing {file}...\")\n",
    "        adata = sc.read(os.path.join(output_dir, file))\n",
    "        \n",
    "        if merged_adata is None:\n",
    "            merged_adata = adata\n",
    "        else:\n",
    "            merged_adata = sc.concat([merged_adata, adata])\n",
    "        \n",
    "        del adata\n",
    "        gc.collect()\n",
    "\n",
    "# Save the final merged AnnData object\n",
    "merged_adata.write(os.path.join(output_dir, 'merged_data.h5ad'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.filter_genes(merged_adata, min_cells=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
